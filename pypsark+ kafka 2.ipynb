{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcd135de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent{'id': 1, 'value': 'message1'}\n",
      "sent{'id': 2, 'value': 'message2'}\n",
      "sent{'id': 3, 'value': 'message3'}\n",
      "sent{'id': 4, 'value': 'message4'}\n",
      "sent{'id': 5, 'value': 'message5'}\n",
      "sent{'id': 6, 'value': 'message6'}\n",
      "sent{'id': 7, 'value': 'message7'}\n",
      "sent{'id': 8, 'value': 'message8'}\n",
      "sent{'id': 9, 'value': 'message9'}\n",
      "sent{'id': 10, 'value': 'message10'}\n",
      "sent{'id': 11, 'value': 'message11'}\n",
      "sent{'id': 12, 'value': 'message12'}\n",
      "sent{'id': 13, 'value': 'message13'}\n",
      "sent{'id': 14, 'value': 'message14'}\n",
      "sent{'id': 15, 'value': 'message15'}\n",
      "sent{'id': 16, 'value': 'message16'}\n",
      "sent{'id': 17, 'value': 'message17'}\n",
      "sent{'id': 18, 'value': 'message18'}\n",
      "sent{'id': 19, 'value': 'message19'}\n"
     ]
    }
   ],
   "source": [
    "from kafka import KafkaProducer\n",
    "import time \n",
    "import json\n",
    "\n",
    "producer = KafkaProducer(bootstrap_servers='localhost:9092',\n",
    "                         value_serializer=lambda v:\n",
    "                         json.dumps(v).encode('utf-8')\n",
    ")\n",
    "\n",
    "# هنعمل توليد لرساله كل ثانيه تتبعت ل التوبيك الي هو spark-topic\n",
    "for i in range(1,20):\n",
    "    data ={\"id\":i,\"value\":f\"message{i}\"}\n",
    "    producer.send('spark-topic',value=data)\n",
    "    print(f\"sent{data}\")\n",
    "\n",
    "\n",
    "\n",
    "producer.flush() # يتاكد ان كل البافرز اتبعت فعلا  \n",
    "producer.close()# يقفل الاتصال نظيف \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59557742",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/11/12 00:36:21 WARN Utils: Your hostname, ahmed-refat-VirtualBox, resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "25/11/12 00:36:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /home/ahmed-refat/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /home/ahmed-refat/.ivy2.5.2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-ad719748-2c85-42a2-955e-c9326c42fa43;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.13;4.0.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.13;4.0.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.9.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.7 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.16 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.4.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.4.1 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.scala-lang.modules#scala-parallel-collections_2.13;1.2.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.12.0 in central\n",
      ":: resolution report :: resolve 3263ms :: artifacts dl 154ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.12.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.4.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.4.1 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.9.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.13;4.0.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.13;4.0.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.scala-lang.modules#scala-parallel-collections_2.13;1.2.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.16 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.7 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-ad719748-2c85-42a2-955e-c9326c42fa43\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 11 already retrieved (0kB/50ms)\n",
      "25/11/12 00:36:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/12 00:37:00 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-35a4734c-89a9-4ad5-928a-c832f24fe41c. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/11/12 00:37:00 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+---+---------+\n",
      "| id|    value|\n",
      "+---+---------+\n",
      "|  1| message1|\n",
      "|  2| message2|\n",
      "|  3| message3|\n",
      "|  4| message4|\n",
      "|  5| message5|\n",
      "|  6| message6|\n",
      "|  7| message7|\n",
      "|  8| message8|\n",
      "|  9| message9|\n",
      "| 10|message10|\n",
      "| 11|message11|\n",
      "| 12|message12|\n",
      "| 13|message13|\n",
      "| 14|message14|\n",
      "| 15|message15|\n",
      "| 16|message16|\n",
      "| 17|message17|\n",
      "| 18|message18|\n",
      "| 19|message19|\n",
      "|  1| message1|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/12 21:47:41 WARN KafkaOffsetReaderAdmin: Error in attempt 1 getting Kafka offsets: \n",
      "java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Call(callName=describeTopics, deadlineMs=1762905829741, tries=1, nextAllowedTryMs=1762976861621) timed out at 1762976861534 after 1 attempt(s)\n",
      "\tat java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:396)\n",
      "\tat java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2073)\n",
      "\tat org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)\n",
      "\tat org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:65)\n",
      "\tat org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:64)\n",
      "\tat org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:101)\n",
      "\tat org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:112)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:521)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:540)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:520)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchLatestOffsets(KafkaOffsetReaderAdmin.scala:305)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.latestOffset(KafkaMicroBatchStream.scala:132)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:618)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressContext.reportTimeTaken(ProgressReporter.scala:186)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:617)\n",
      "\tat scala.collection.immutable.List.map(List.scala:247)\n",
      "\tat scala.collection.immutable.List.map(List.scala:79)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:606)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.scala:17)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:1011)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:602)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$executeOneBatch$2(MicroBatchExecution.scala:375)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressContext.reportTimeTaken(ProgressReporter.scala:186)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.executeOneBatch(MicroBatchExecution.scala:364)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:344)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1$adapted(MicroBatchExecution.scala:344)\n",
      "\tat org.apache.spark.sql.execution.streaming.TriggerExecutor.runOneBatch(TriggerExecutor.scala:39)\n",
      "\tat org.apache.spark.sql.execution.streaming.TriggerExecutor.runOneBatch$(TriggerExecutor.scala:37)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.runOneBatch(TriggerExecutor.scala:70)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:82)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:344)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:337)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:311)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:226)\n",
      "Caused by: org.apache.kafka.common.errors.TimeoutException: Call(callName=describeTopics, deadlineMs=1762905829741, tries=1, nextAllowedTryMs=1762976861621) timed out at 1762976861534 after 1 attempt(s)\n",
      "Caused by: org.apache.kafka.common.errors.DisconnectException: Cancelled describeTopics request with correlation id 1361549 due to node 1 being disconnected\n"
     ]
    }
   ],
   "source": [
    "# هنعمل بقا كونسيومر علي الداتا دي ونعمل عليها بروسسنج في سبارك \n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json,col\n",
    "from pyspark.sql.types import StringType ,StructType ,IntegerType\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    ".appName(\"Kafkasparkexample\")\\\n",
    ".getOrCreate()\n",
    "\n",
    "\n",
    "#    schema عمليه بناء ال \n",
    "schema= StructType() \\\n",
    "    .add(\"id\",IntegerType()) \\\n",
    "    .add(\"value\",StringType())\n",
    "\n",
    "\n",
    "#  قراءه الاستريم من كاكفا \n",
    "df= spark.readStream .format(\"kafka\") \\\n",
    "    .option (\"kafka.bootstrap.servers\",\"localhost:9092\")\\\n",
    "    .option(\"subscribe\",\"spark-topic\")\\\n",
    "    .option(\"startingoffsets\",\"earliest\")\\\n",
    "    .load()\n",
    "\n",
    "\n",
    "#  من بايت ل جيسون value  هنعمل تحويل ال  \n",
    "json_df= df.select (from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\")).select(\"data.*\")\n",
    "\n",
    "\n",
    "# هنعرض الداتا بقا في الكونسول \n",
    "query = json_df.writeStream \\\n",
    ".outputMode(\"append\")\\\n",
    ".format(\"console\")\\\n",
    ".start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ba85c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
