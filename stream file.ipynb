{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22266561",
   "metadata": {},
   "source": [
    "Ø§Ù„ÙÙƒØ±Ø© Ø¨Ø¨Ø³Ø§Ø·Ø©\n",
    "\n",
    "ÙÙŠ File StreamingØŒ Spark Ø¨ÙŠØ¹Ù…Ù„  Ø¹Ù„Ù‰ ÙÙˆÙ„Ø¯Ø± Ù…Ø¹ÙŠÙ†.\n",
    "ÙƒÙ„ Ù…Ø§ ØªØ­Ø· Ù…Ù„Ù Ø¬Ø¯ÙŠØ¯ ÙÙŠ Ø§Ù„ÙÙˆÙ„Ø¯Ø± Ø¯Ù‡ØŒ Spark ÙŠÙ‚Ø±Ø£Ù‡ ØªÙ„Ù‚Ø§Ø¦ÙŠ ÙˆÙŠØ¹Ù…Ù„ Ø¹Ù„ÙŠÙ‡ Ø§Ù„Ù€ Processing Ø§Ù„Ù„ÙŠ Ø§Ù†Øª Ù…Ø­Ø¯Ø¯Ù‡Ø§.\n",
    "\n",
    "ğŸ’¡ ÙŠØ¹Ù†ÙŠ Ø¨Ø¯Ù„ Ù…Ø§ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¬Ø§ÙŠØ© Ù…Ù† socket liveØŒ Ù‡ÙŠ Ø¬Ø§ÙŠØ© Ù…Ù† Ù…Ù„ÙØ§Øª Ø¬Ø¯ÙŠØ¯Ø© Ø¨ØªØªØ­Ø· ÙÙŠ ÙÙˆÙ„Ø¯Ø± Ù…Ø¹ÙŠÙ†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44897019",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/10/31 20:26:16 WARN Utils: Your hostname, ahmed-refat-VirtualBox, resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "25/10/31 20:26:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/31 20:26:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# stream from files \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, sum, count\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"StreamingOrdersExample\") \\\n",
    "    .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931d0b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer: string (nullable = true)\n",
      " |-- product: string (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Ø§Ù„Ø§ÙˆÙ„ Ù‡Ù†Ø­Ø¯Ø¯ Ø§Ù„Ø§Ø³ÙƒÙ…ÙŠØ§ Ø¨ØªØ§Ø¹ØªÙ†Ø§ \n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"customer\", StringType(), True),\n",
    "    StructField(\"product\", StringType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"price\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„ÙØ§Øª ÙƒÙ€ stream\n",
    "orders = spark.readStream \\\n",
    "    .option(\"header\", True) \\\n",
    "    .schema(schema) \\\n",
    "    .csv(\"/home/ahmed-refat/datastream\")  # â† Ø§Ø³Ù… Ø§Ù„ÙÙˆÙ„Ø¯Ø±\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Ø·Ø¨Ø§Ø¹Ø© schema Ù„Ù„ØªØ£ÙƒØ¯\n",
    "orders.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10002970",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/31 21:46:01 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-4b8237d4-b4aa-4921-8c78-8cde85578323. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/10/31 21:46:01 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[STREAMING_OUTPUT_MODE.UNSUPPORTED_OPERATION] Invalid streaming output mode: complete. This output mode is not supported for no streaming aggregations on streaming DataFrames/DataSets. SQLSTATE: 42KDE",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Ø¹Ø±Ø¶ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¹Ù„Ù‰ console ÙƒÙ„ 5 Ø«ÙˆØ§Ù†ÙŠ\u001b[39;00m\n\u001b[1;32m      2\u001b[0m query \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      3\u001b[0m     \u001b[43morders\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteStream\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputMode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Ø¹Ø±Ø¶ Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© ÙÙ‚Ø·\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconsole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ Ø§Ù„Ù€ console\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrigger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessingTime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m10 seconds\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# ØªØ­Ø¯ÙŠØ« ÙƒÙ„ 5 Ø«ÙˆØ§Ù†ÙŠ\u001b[39;49;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/pyspark_env/lib/python3.10/site-packages/pyspark/sql/streaming/readwriter.py:1704\u001b[0m, in \u001b[0;36mDataStreamWriter.start\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[1;32m   1702\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueryName(queryName)\n\u001b[1;32m   1703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1704\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1705\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1706\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39mstart(path))\n",
      "File \u001b[0;32m~/miniconda3/envs/pyspark_env/lib/python3.10/site-packages/py4j/java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/pyspark_env/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:288\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    284\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [STREAMING_OUTPUT_MODE.UNSUPPORTED_OPERATION] Invalid streaming output mode: complete. This output mode is not supported for no streaming aggregations on streaming DataFrames/DataSets. SQLSTATE: 42KDE"
     ]
    }
   ],
   "source": [
    "# Ø¹Ø±Ø¶ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¹Ù„Ù‰ console ÙƒÙ„ 5 Ø«ÙˆØ§Ù†ÙŠ\n",
    "query = (\n",
    "    orders.writeStream\n",
    "    .outputMode(\"complete\")        # Ø¹Ø±Ø¶ Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© ÙÙ‚Ø·\n",
    "    .format(\"console\")           # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ Ø§Ù„Ù€ console\n",
    "    .trigger(processingTime='10 seconds')  # ØªØ­Ø¯ÙŠØ« ÙƒÙ„ 5 Ø«ÙˆØ§Ù†ÙŠ\n",
    "    .start()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0b76055",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/31 21:44:32 WARN DAGScheduler: Failed to cancel job group 6163b4e3-2b4a-4ef2-b8db-3fb2a3bdc2c1. Cannot find active jobs for it.\n",
      "25/10/31 21:44:33 WARN DAGScheduler: Failed to cancel job group 6163b4e3-2b4a-4ef2-b8db-3fb2a3bdc2c1. Cannot find active jobs for it.\n"
     ]
    }
   ],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a451d324",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
